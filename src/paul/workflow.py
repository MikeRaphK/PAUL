from .pytest_tool import pytest_tool
from .react_graph import build_react_graph
from langchain_community.callbacks.openai_info import OpenAICallbackHandler
from langchain_community.tools import ReadFileTool, WriteFileTool, ListDirectoryTool
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from typing import Optional, Dict, List

import json
import os


def parse_paul_response(
    chat_history: List[BaseMessage],
    token_logger: OpenAICallbackHandler,
    issue_number: Optional[int] = None,
) -> Dict[str, str]:
    """
    Parse JSON response from PAUL.

    Args:
        chat_history (List[BaseMessage]): The conversation history with the LLM, where the last message contains the JSON response.
        token_logger (OpenAICallbackHandler): The callback object containing LLM usage statistics.
        issue_number (Optional[int]): The GitHub issue number for reference in the PR body. Defaults to None.

    Returns:
        Dict[str, str]: The enriched JSON object containing PR details and additional metadata.
    """
    paul_response = json.loads(chat_history[-1].content)

    # Add starting note
    paul_response["pr_body"] = (
        "> **Note:** This message was automatically generated by PAUL. Please review the proposed changes carefully before merging.\n\n"
        + paul_response["pr_body"]
        + "\n\n"
    )

    # Add tool usage info
    paul_response["pr_body"] += f"Tools Used:\n"
    tool_str = ""
    for message in chat_history:
        tool_calls = getattr(message, "additional_kwargs", {}).get("tool_calls", [])
        for tool_call in tool_calls:
            tool_name = tool_call["function"]["name"]
            tool_arguments = tool_call["function"]["arguments"]
            tool_str += f"- `{tool_name}` with arguments `{tool_arguments}`\n"
    if tool_str:
        paul_response["pr_body"] += f"{tool_str}\n"
    else:
        paul_response["pr_body"] += "None\n\n"

    # Add token usage info
    paul_response["pr_body"] += f"Tokens Used: {token_logger.total_tokens}\n"
    paul_response[
        "pr_body"
    ] += f"Successful Requests: {token_logger.successful_requests}\n"
    paul_response["pr_body"] += f"Total Cost (USD): {token_logger.total_cost:.6f}\n"

    # Add ending note
    if issue_number is not None:
        paul_response["pr_body"] += f"\nRelated to #{issue_number}\n"

    return paul_response


def run_paul_workflow(
    *,
    repo_path: str,
    issue_body: str,
    OPENAI_API_KEY: str,
    issue_title: Optional[str] = None,
    issue_number: Optional[int] = None,
    model: str = "gpt-4o-mini",
) -> Dict[str, str]:
    """
    Executes the PAUL workflow to generate a patch for a given issue using an LLM agent.

    Args:
        repo_path (str): Path to the local repository.
        issue_body (str): Body/description of the issue to resolve.
        openai_api_key (str): The OpenAI API key.
        issue_title (Optional[str]): Title of the issue to resolve. Defaults to None.
        issue_number (Optional[int]): Issue number (for response parsing). Defaults to None.
        model (str): The OpenAI model name to use. Defaults to 'gpt-4o-mini'.

    Returns:
        Dict[str, str]: Dict with the PAUL's JSON response containing: 'commit_msg', 'pr_title', 'pr_body'.

    Raises:
        RuntimeError: If the workflow fails due to recursion limit.
    """
    print("Waking PAUL up...\n")

    START_DIR = os.getcwd()
    GRAPH_PNG_PATH = os.path.join(START_DIR, "src/paul/resources/graph.png")
    SYSTEM_MESSAGE_PATH = os.path.join(
        START_DIR, "src/paul/resources/system_message.txt"
    )
    os.chdir(repo_path)

    print(f"Initializing ReAct graph using '{model}'...\n")
    token_logger = OpenAICallbackHandler()
    llm = ChatOpenAI(
        model=model, openai_api_key=OPENAI_API_KEY, callbacks=[token_logger]
    )
    tools = [ReadFileTool(), WriteFileTool(), ListDirectoryTool(), pytest_tool]
    # PAUL = build_react_graph(tools, llm, GRAPH_PNG_PATH)
    PAUL = create_react_agent(model=llm, tools=tools)

    print("Invoking PAUL...\n")
    with open(SYSTEM_MESSAGE_PATH, "r") as f:
        system_message = SystemMessage(content=f.read())
    chat_history = [system_message]

    query = ""
    if issue_title is not None:
        query += f"Issue Title: {issue_title}\n"
    query += f"Issue Body: {issue_body}"
    chat_history.append(HumanMessage(content=query))

    output_state = PAUL.invoke({"messages": chat_history}, config={"recursion_limit": 50})

    print("PAUL has finished working!\n")
    os.chdir(START_DIR)
    return parse_paul_response(output_state["messages"], token_logger, issue_number)
