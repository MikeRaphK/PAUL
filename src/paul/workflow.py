from .graph import build_paul_graph
from langchain_community.callbacks.openai_info import OpenAICallbackHandler
from langchain_community.tools import ReadFileTool, WriteFileTool, ListDirectoryTool
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI
from typing import Optional, Dict, List

import json
import os

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
RESOURCES_DIR = os.path.join(SCRIPT_DIR, "resources")
SYSTEM_MESSAGE_PATH = os.path.join(RESOURCES_DIR, "system_message.txt")


def parse_paul_response(
    chat_history: List[BaseMessage],
    token_logger: OpenAICallbackHandler,
    issue_number: Optional[int] = None,
) -> Dict[str, str]:
    """
    Parse JSON response from PAUL.

    Args:
        chat_history (List[BaseMessage]): The conversation history with the LLM, where the last message contains the JSON response.
        token_logger (OpenAICallbackHandler): The callback object containing LLM usage statistics.
        issue_number (Optional[int]): The GitHub issue number for reference in the PR body. Defaults to None.

    Returns:
        Dict[str, str]: The enriched JSON object containing PR details and additional metadata.
    """
    paul_response = json.loads(chat_history[-1].content)

    # Add starting note
    paul_response["pr_body"] = (
        "> **Note:** This message was automatically generated by PAUL. Please review the proposed changes carefully before merging.\n\n"
        + paul_response["pr_body"]
        + "\n\n"
    )

    # Add tool usage info
    paul_response["pr_body"] += f"Tools Used:\n"
    tool_str = ""
    for message in chat_history:
        tool_calls = getattr(message, "additional_kwargs", {}).get("tool_calls", [])
        for tool_call in tool_calls:
            tool_name = tool_call["function"]["name"]
            tool_arguments = tool_call["function"]["arguments"]
            tool_str += f"- `{tool_name}` with arguments `{tool_arguments}`\n"
    if tool_str:
        paul_response["pr_body"] += f"{tool_str}\n"
    else:
        paul_response["pr_body"] += "None\n\n"

    # Add token usage info
    paul_response["pr_body"] += f"Tokens Used: {token_logger.total_tokens}\n"
    paul_response[
        "pr_body"
    ] += f"Successful Requests: {token_logger.successful_requests}\n"
    paul_response["pr_body"] += f"Total Cost (USD): {token_logger.total_cost:.6f}\n"

    # Add ending note
    if issue_number is not None:
        paul_response["pr_body"] += f"\nRelated to #{issue_number}\n"

    return paul_response


def run_paul_workflow(
    *,
    repo_path: str,
    issue_body: str,
    OPENAI_API_KEY: str,
    issue_title: Optional[str] = None,
    issue_number: Optional[int] = None,
    model: str,
    tests: list[str]
) -> Dict[str, str]:
    """
    Executes the PAUL workflow to generate a patch for a given issue using an LLM agent.

    Args:
        repo_path (str): Path to the local repository.
        issue_body (str): Body/description of the issue to resolve.
        openai_api_key (str): The OpenAI API key.
        issue_title (Optional[str]): Title of the issue to resolve. Defaults to None.
        issue_number (Optional[int]): Issue number (for response parsing). Defaults to None.
        model (str): The OpenAI model name to use.

    Returns:
        Dict[str, str]: Dict with the PAUL's JSON response containing: 'commit_msg', 'pr_title', 'pr_body'.

    Raises:
        RuntimeError: If the workflow fails due to recursion limit.
    """
    absolute_tests = []
    for test in tests:
        absolute_test = os.path.abspath(test)
        print(f"Will run test: '{absolute_test}'\n")
        absolute_tests.append(absolute_test)
    START_DIR = os.getcwd()
    os.chdir(repo_path)

    print(f"Building PAUL using '{model}'...\n")
    token_logger = OpenAICallbackHandler()
    llm = ChatOpenAI(model="gpt-4o-mini", openai_api_key=OPENAI_API_KEY, callbacks=[token_logger])
    toolkit = [ReadFileTool(), WriteFileTool(), ListDirectoryTool()]
    llm_with_tools = llm.bind_tools(toolkit)
    PAUL = build_paul_graph(toolkit)

    print("Building initial state...\n")
    with open(SYSTEM_MESSAGE_PATH, "r") as f:
        system_message = SystemMessage(content=f.read())
    query = ""
    if issue_title is not None:
        query += f"Issue Title: {issue_title}\n"
    query += f"Issue Body: {issue_body}"
    chat_history = [system_message, HumanMessage(content=query)]
    initial_state = {
        "messages": chat_history,
        "llm": llm_with_tools,
        "tests": absolute_tests,
        "tests_pass": False
    }

    print("Working on a patch...\n")
    output_state = PAUL.invoke(initial_state, config={"recursion_limit": 50})

    print("PAUL has finished working!\n")
    os.chdir(START_DIR)
    return parse_paul_response(output_state["messages"], token_logger, issue_number)
